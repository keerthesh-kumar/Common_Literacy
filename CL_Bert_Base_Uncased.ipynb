{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CL_Bert_Base_Uncased.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOWVrIaoDLpieF6H5Q5qmt3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"U-CNCfA8HtlR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626700001944,"user_tz":-330,"elapsed":697,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"dae3a423-7128-468f-a259-da189f6c1ed6"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RR8svdLND7U2","executionInfo":{"status":"ok","timestamp":1626700005264,"user_tz":-330,"elapsed":2742,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"bc3edd16-e1ad-47a7-c560-fa4e76ce7003"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sE6Y6-EH265","executionInfo":{"status":"ok","timestamp":1626700005265,"user_tz":-330,"elapsed":12,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"b20c9cdd-9320-40d2-b915-fec7bd1f6bfd"},"source":["!ls \"/content/drive/MyDrive/DeepLearning/Common Literacy\""],"execution_count":null,"outputs":[{"output_type":"stream","text":[" Common_Literacy_AVGW2V_POS.ipynb\n"," Common_Literacy_AVGW2V_TFIDFAVGW2V_POS.ipynb\n"," CommonLiteracyBertModels.ipynb\n"," CommonLiteracyBertModels_VV.ipynb\n"," Common_Literacy_BOW_TFIDF_AvgW2V100D.ipynb\n"," Common_Literacy_BOW_TFIDF_AvgW2V100D_SVD.ipynb\n"," Common_Literacy_BOW_TFIDF_AVGW2V300D.ipynb\n"," CommonLiteracy_CNN_Model.ipynb\n"," CommonLiteracy_CNN_Model_Word2Vec_TFIDF.ipynb\n","'Copy of CommonLiteracyBertModels_VV.ipynb'\n"," Dataset\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5CN1ggCJIPT5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626700007907,"user_tz":-330,"elapsed":2647,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"86fe2b60-533a-494e-cd85-52c4a231d330"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","from tqdm import tqdm\n","import seaborn as sns\n","\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","\n","%matplotlib inline\n","\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","# specify GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"z7a8dmElIk7Z"},"source":["train ='/content/drive/MyDrive/DeepLearning/Common Literacy/Dataset/train.csv'\n","test = '/content/drive/MyDrive/DeepLearning/Common Literacy/Dataset/test.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkNdVejynRcQ"},"source":["# only loading the train data as test data is not complete\n","df_train = pd.read_csv(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8d1d7GeQxjLw"},"source":["# Defining different parameters of the code\n","MAX_LEN = 350\n","BATCH_SIZE = 8\n","PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n","EPOCHS = 16"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJtyDdOlEiej"},"source":["### Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"1r2ST4DFImjy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626700011333,"user_tz":-330,"elapsed":3431,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"fe78df6c-178a-4913-b066-5bc536cec20e"},"source":["# Getting the bert base uncased model from transformers for tokenizing\n","bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"iknm0Zbm7Kc-"},"source":["### Special Tokens Name and IDs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnkK8lYV7Jnl","executionInfo":{"status":"ok","timestamp":1626700011334,"user_tz":-330,"elapsed":34,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"2c4e817a-c402-4f85-d27f-81d03016854e"},"source":["print(\"This is SEP token:\",tokenizer.sep_token, tokenizer.sep_token_id)\n","print(\"This is CLS token:\",tokenizer.cls_token, tokenizer.cls_token_id)\n","print(\"This is PAD token:\",tokenizer.pad_token, tokenizer.pad_token_id)\n","print(\"This is UNK token:\",tokenizer.unk_token, tokenizer.unk_token_id)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is SEP token: [SEP] 102\n","This is CLS token: [CLS] 101\n","This is PAD token: [PAD] 0\n","This is UNK token: [UNK] 100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tp3lJ5tHAu1Y"},"source":["### Choosing Sequence Length"]},{"cell_type":"code","metadata":{"id":"YCOVSJHCKc9t"},"source":["class Common_Literacy_Dataset(Dataset):\n","\n","  def __init__(self, reviews, targets, tokenizer, max_len):\n","    self.reviews = reviews\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","  \n","  def __len__(self):\n","    return len(self.reviews)\n","  \n","  def __getitem__(self, item):\n","    review = str(self.reviews[item])\n","    target = self.targets[item]\n","\n","    encoding = self.tokenizer.encode_plus(\n","      review,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      padding='max_length',\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","\n","    return {\n","      'review_text': review,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'targets': torch.tensor(target)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHqsFxBpLJH8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626700011336,"user_tz":-330,"elapsed":29,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"30aeffb0-c821-4cae-ad04-f91147edb71f"},"source":["df = df_train[['target','excerpt']]\n","print(df.columns)\n","\n","CL_train, CL_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n","CL_train, CL_val = train_test_split(CL_train, test_size=0.2, random_state=RANDOM_SEED)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Index(['target', 'excerpt'], dtype='object')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xpFzCeuKpcX","executionInfo":{"status":"ok","timestamp":1626700011337,"user_tz":-330,"elapsed":26,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"f7a64f3e-c66c-43a2-cd99-d09ca22305c7"},"source":["CL_train.shape, CL_val.shape, CL_test.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1813, 2), (454, 2), (567, 2))"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"JF_qgFXwL1me"},"source":["def create_data_loader(df, tokenizer, max_len, batch_size):\n","  ds = Common_Literacy_Dataset(\n","    reviews=df['excerpt'].to_numpy(),\n","    targets=df['target'].to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","  )\n","\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=4\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQlB74VRKAgK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626700011340,"user_tz":-330,"elapsed":25,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"e44aa289-2339-46cb-e1d4-f673c300323b"},"source":["train_data_loader = create_data_loader(CL_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(CL_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(CL_test, tokenizer, MAX_LEN, BATCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"QDesroKfX6jF"},"source":["class Common_Literacy_Regressor(nn.Module):\n","\n","  def __init__(self):\n","    super(Common_Literacy_Regressor, self).__init__()\n","    self.bert = bert_model\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(in_features=self.bert.config.hidden_size,out_features=1)\n","    self.double()\n","  \n","  def forward(self, input_ids, attention_mask):\n","    _, pooled_output = self.bert(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask,\n","      return_dict=False\n","    )\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6uYnl0OYmzo"},"source":["model = Common_Literacy_Regressor()\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tF5ab_VpZYHj"},"source":["loss_fn = nn.MSELoss(reduction='mean').to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NedRwXO_dF_8"},"source":["def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","  model = model.train()\n","\n","  train_losses = 0\n","  no_train_steps = 0\n","\n","  for d in data_loader:\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].to(device)\n","\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","\n","    # print(\"=\"*100)\n","\n","    loss = loss_fn(outputs, targets)\n","    # print(\"This is training Loss:\",loss)\n","    train_losses += loss.item()\n","\n","    # print(\"=\"*100)\n","\n","    no_train_steps += 1\n","\n","    # print(\"Number of Steps:\",no_train_steps)\n","    # print(\"=\"*100)\n","\n","    loss.backward()\n","\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","\n","  return train_losses, no_train_steps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYh4snu3H8zZ"},"source":["def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  val_losses = 0\n","  no_val_steps = 0\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","\n","      # print(\"=\"*100)\n","\n","      loss = loss_fn(outputs, targets)\n","      # print(\"This is Evaluation Loss:\",loss)\n","\n","      # print(\"=\"*100)\n","\n","      val_losses += loss.item()\n","\n","      no_val_steps += 1\n","\n","      # print(\"Number of Steps:\",no_val_steps)\n","      # print(\"=\"*100)\n","\n","  return val_losses, no_val_steps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tFbdQCajIKly","executionInfo":{"status":"error","timestamp":1626703088437,"user_tz":-330,"elapsed":638282,"user":{"displayName":"Keerthesh Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAlWru4SVVX2aCzpl0my2Wti22s5049U7fDu709A=s64","userId":"01248380491650672453"}},"outputId":"438e914b-bbaa-497e-815e-f4dc5d29c972"},"source":["from collections import defaultdict\n","\n","history = defaultdict(list)\n","best_rmse = 0\n","\n","for epoch in range(EPOCHS):\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","\n","  train_losses, no_train_steps = train_epoch(\n","    model,\n","    train_data_loader,    \n","    loss_fn, \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(CL_train)\n","  )\n","\n","  MSE_train = train_losses/no_train_steps\n","  RMSE_train = np.sqrt(mean_squared_error_train)\n","\n","  print(f'Train MSE {MSE_train} RSME {RMSE_train}')\n","\n","  val_losses, no_val_steps = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn, \n","    device, \n","    len(CL_val)\n","  )\n","\n","  MSE_val = val_losses/no_val_steps\n","  RMSE_val = np.sqrt(MSE_val)\n","\n","  print(f'Validation MSE {MSE_val} RSME {RMSE_val}')\n","  \n","  history['train_MSE'].append(MSE_train)\n","  history['train_RMSE'].append(RMSE_train)\n","  history['val_MSE'].append(MSE_val)\n","  history['val_RMSE'].append(RMSE_val)\n","\n","  if RMSE_val > best_rmse:\n","    torch.save(model.state_dict(), 'best_model_state.bin')\n","    best_rmse = RMSE_val"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/16\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Train MSE 0.9569834749103096 RSME 0.979500395394024\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Validation MSE 1.1254132575391411 RSME 1.0608549653647954\n","Epoch 2/16\n","----------\n","Train MSE 0.9577925230467235 RSME 0.979500395394024\n","Validation MSE 1.1254132575391411 RSME 1.0608549653647954\n","Epoch 3/16\n","----------\n","Train MSE 0.9573438319475264 RSME 0.979500395394024\n","Validation MSE 1.1254132575391411 RSME 1.0608549653647954\n","Epoch 4/16\n","----------\n","Train MSE 0.9572165232665533 RSME 0.979500395394024\n","Validation MSE 1.1254132575391411 RSME 1.0608549653647954\n","Epoch 5/16\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f894c2aedd0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1301, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 140, in join\n","    res = self._popen.wait(timeout)\n","  File \"/usr/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n","    if not wait([self.sentinel], timeout):\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n","    fd_event_list = self._selector.poll(timeout)\n","KeyboardInterrupt: \n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-ada4af70edf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCL_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-4666bd4c2671>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             raise RuntimeError(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"rSDCG9sQFtHc"},"source":["plt.plot(history['train_RMSE'], label='train RMSE')\n","plt.plot(history['val_RMSE'], label='validation RMSE')\n","\n","plt.title('Training history')\n","plt.ylabel('RMSE')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1]);"],"execution_count":null,"outputs":[]}]}